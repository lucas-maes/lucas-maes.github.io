---
layout: page
---

<span class="pp" style="display:block; margin-top:-30px;">
![My face](https://avatars.githubusercontent.com/u/43337476?s=400&u=935f09908f80e0f67374ae1f8d4f6f0c6ee39c17&v=4)
</span>

<div style="text-align:center; width: 350px; margin-bottom: 10px;">
    <ul class="sub-nav">
        <li class="sub-item"><a href="https://github.com/lucas-maes" target="_blank">Github</a></li>
        <li class="sub-item"><a href="https://twitter.com/lucasmaes_" target="_blank">Twitter</a></li>
        <li class="sub-item"><a href="mailto:lucas.maes@mila.quebec">Email</a></li>
    </ul>
    <span class="email"> lucas.maes [AT] mila.quebec </span>
</div>

## About me

I am a second year PhD student at [Mila](https://www.mila.quebec/") and the Université de Montréal, supervised by [Damien Scieur](https://damienscieur.com/).

I am interested in **practical deep learning theory** (and the opposite). My ambition is to develop relevant theory for industrial-scale deep learning. My main approach is through the lens of optimization, aiming to improve the efficiency, cost-effectiveness, and robustness of deep learning models with guarantees. Additionally, I am also focused on building theoretical frameworks to explain the empirical behavior of deep learning, with a particular interest in self-supervised learning.

### Must Read Papers

Below is a selection of the most impactful papers I've read that have significantly shaped my research interests and vision of deep learning. These works have not only inspired me intellectually but also helped set my priorities on where research efforts should be focused. I hope they resonate with you as they have with me.

| [On the Geometry of Deep Learning](https://arxiv.org/pdf/2408.04809)                                          |
| [On the Limitations of Elo: Real-World Games, are Transitive, not Additive](https://arxiv.org/abs/2206.12301) |
| [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507)     |
| [VICReg: Variance-Invariance-Covariance Regularization For Self-Supervised Learning](https://arxiv.org/abs/2105.04906) |

## News

- Sep 2024: We launched [Mila Optim CrashCourse v2](https://mila-optim.github.io/).
- Jan 2024: We launched [Mila Optim CrashCourse](https://mila-optim.github.io/) a peer-to-peer lecture series on optimization.
- Jan 2024: I joined [MTLMlOpt](https://mtl-mlopt.github.io/) as an organizer.
- Sep 2023: I am starting my PhD at Mila and the Université de Montréal.

<p class="copyright"> Theme from <a href="https://atcold.github.io/">Alfredo Canziani</a>.  <span class="last-edit" style='float:right;'>Last update: 08 Sep 2024.</span></p>
